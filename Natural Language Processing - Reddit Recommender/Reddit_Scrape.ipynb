{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import requests\n",
    "import requests.auth\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is my token generator (info replaced by placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': '63606394-pEd5pskuXXJ5QkwpZ6GLFUW-Pbg',\n",
       " 'token_type': 'bearer',\n",
       " 'expires_in': 3600,\n",
       " 'scope': '*'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_auth = requests.auth.HTTPBasicAuth('placeholder_key1', 'placeholder_key2')\n",
    "post_data = {\"grant_type\": \"password\", \"username\": \"placeholder_lawrence\", \"password\": \"placeholder_password\"}\n",
    "headers = {\"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n",
    "response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Requests for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting my own account data\n",
    "\n",
    "headers = {\"Authorization\": \"bearer 63606394-pEd5pskuXXJ5QkwpZ6GLFUW-Pbg\", \"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n",
    "response = requests.get(\"https://oauth.reddit.com/api/v1/me\", headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a subreddit's info\n",
    "\n",
    "headers = {\"Authorization\": \"bearer 63606394-pEd5pskuXXJ5QkwpZ6GLFUW-Pbg\", \"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n",
    "response = requests.get(\"https://oauth.reddit.com/r/funny/api/info\", headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a praw reddit instance with my reddit bot information\n",
    "\n",
    "reddit = praw.Reddit(client_id='wLcyy65pcFCyoA', \\\n",
    "                     client_secret='placeholder_secret', \\\n",
    "                     user_agent='Project', \\\n",
    "                     username='placeholder_lawrence', \\\n",
    "                     password='placeholder_password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the text of top X posts of all time from a subreddit\n",
    "\n",
    "def top_posts(sub, number_posts):\n",
    "    \n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    post_text = []\n",
    "    for submission in subreddit.top(limit=number_posts): \n",
    "        post_text.append(submission.title)\n",
    "    return post_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TIFU by buying everyone an AncestryDNA kit and ruining Christmas',\n",
       " 'TIFU by living in a dark bedroom for 6 years.',\n",
       " 'TIFU by spending the last year on reddit talking to myself after being muted.',\n",
       " 'TIFU by reporting a pervert to the School my sons go to.',\n",
       " '/r/tifu last weekend']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_posts('tifu', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to make good humored inappropriate jokes about his lollipop, but ultimately I just want to say that I am stoked because he is stoked. \n",
      "Uber hates him\n",
      "He's not just on the cover but also [Mr. December](https://i.imgur.com/J4wQbxf.png)\n",
      "\n",
      "\n",
      "Here's the [whole calendar](https://nyctaxicalendar.com/) which features plenty of shirtless NYC cab dudes.\n",
      "\n",
      "Where can I buy this? It’s my birthday tomorrow and I’d love to gift myself something nice. \n",
      "I love random adorable shit like this. I'd never in a million years expect to see this yet here I am grinning ear to ear because a yellow cab driver in New York took a shirtless picture licking a lollipop. \n",
      "I’m feeling a lot of conflicting emotions right now. \n",
      "makes me wonder how many average normal people we encounter are actually fabulous when not working\n",
      "Welp, then to the frontpage you go!\n",
      "It's like Jackie Chan and Tommy Wiseau had a son\n",
      "I got this for a gag gift for a white elephant exchange last year, and even though it wasn’t a competition, I definitely won. \n",
      "But July tho... \n",
      "[deleted]\n",
      "I got this for my sister for Christmas and she was thrilled! That's so cool you got to ride with him!!\n",
      "That's lovely. Here is a tiny doodle of this beautiful man. https://imgur.com/otxFESr\n",
      "42k points in less than 2 hours.\n",
      "\n",
      "In my 9 years those are the most ridiculous numbers I've seen other than the Obama AMA.\n",
      "\n",
      "Wow.\n",
      "Prediction: sold out by morning. This **has** to be vulcanized—or are you a true OP?\n",
      "Whoa, tag NSFW! I coulda lost my job viewing such lewdity.\n",
      "I like how his eye is focused on you. He wants to see your enjoyment.\n",
      "I'd fuck him you cant say you wouldn't fuck him.\n",
      "I'm not gay or anything, but I would pay $20 for that calendar.\n",
      "Now that’s how to attract customers.\n",
      "Buy this calendar at : https://nyctaxicalendar.com \n",
      "Aww this is so cute. Happy for the dude. \n",
      "UPDATE: I was able to contact Alex (the cab driver) and just got off the phone with him. He couldn’t believe the attention he has been getting from the photo!! He wanted me to thank everyone for the overwhelming positive feedback. He also invited me to karaoke later :)\n",
      "This photograph is amazing. The set up cannot be a coincidence. The calendar is highlighted by the cabin light, and everything else is properly exposed too!\n",
      "\n",
      "Correct amount of light on the face of the driver as well. I can see the city outside as it is not darkened. Good contrast. No blurring. Great camera work. This cannot be the work of a phone camera, or is it? I know the tech has come a long way, but kudos to the person behind it.\n",
      "134k upvotes in 4 hours. Gahddamn. \n",
      "Holy shit, I did not expect this to get so much attention! THANK YOU FOR THE GOLD! I have already attempted to reach out to the cab driver to share the news about his overnight fame, and will post an update as soon as I hear back.\n",
      "Last Christmas my wife and I came across this and bought it for her brother.\n",
      "\n",
      "I was absolutely not ashamed to ask the frail old woman at the counter how much the calendar would cost.\n",
      "I wonder how my family would react if my next year's christmas photo was staged like this on my car?\n",
      "He'll be uber happy when this is lyfted to the front page.\n",
      "The pottery dude looks like an actor but I can't quite remember who\n",
      "But why male models?\n",
      "That's one hell of a lollipop, he knows how to spend his tips.\n",
      "166k upvotes?!\n",
      "I’m having a horrible Holiday season and this just made my whole month. Thank the whomever there is still joy like this.\n",
      "Number 1!!!\n",
      "Most upvoted post in reddit history. Wow.\n",
      "Mr. Miyagi seems to have gone down a dark, dark road. \n",
      "This is so adorable. Up to the front page you go!\n",
      "I honestly never thought I’d live to witness all this yes in one picture\n",
      "Just leaving my mark on the top post of all time on Reddit.\n",
      "Looks like this just overtook \"Guardians of the Front Page\" as the most upvoted post of all time.\n",
      "This is a great shot. (Ops pic)\n",
      "\n",
      "I'd show everyone if I was that glamorous as well.\n",
      "How is that site surviving the hug o death.\n",
      "This is so funny and genuinely wholesome \n",
      "I remember about 8 years ago there was an AT&T commercial with people dancing and there was a clip of a cabbie goin dumb hella hard. One night my friends and I are getting drunk and my buddy has to leave so we call a cab to pick him up. It was said cabbie from the commercial and we were stoked. Not in a drunken, arrogant idiotic annoying way; we just thought it was dope to see dude at my neighbor’s house.\n",
      "Starting the year off strong!\n",
      "211k upvotes in 9 hours. Thats crazy!\n",
      "My work has last years version of this calendar and we think it’s the funniest thing ever! Glad to see it really is actual cab drivers and that they’re as fond of it as we are. \n",
      "This made my day. Seriously that is awesome. Thanks OP and thank you cab driver for being badass enough to lick a lolly on a car hood for a photoshoot.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MoreComments' object has no attribute 'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-f8e38d9ed999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtop_level_comment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_level_comment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'MoreComments' object has no attribute 'body'"
     ]
    }
   ],
   "source": [
    "# getting the top level comments of the posts from the previous step\n",
    "\n",
    "for i in funny_submission_ids:\n",
    "    submission = reddit.submission(id = i)\n",
    "    for top_level_comment in submission.comments:\n",
    "        print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_subs = 'announcements, funny, askreddit, todayilearned,science, worldnews,pics, iama, gaming, videos, movies, aww, \\\n",
    "               music, blog, gifs, news, explainlikeimfive, askscience, earthporn, books, television, mildlyinteresting, \\\n",
    "               lifeprotips, showerthoughts, space, diy, jokes, gadgets, nottheonion, sports, tifu, food, photoshopbattles, \\\n",
    "               documentaries, futureology, history, internetisbeautiful, dataisbeautiful, upliftingnews, listentothis, \\\n",
    "               getmotivated, personalfinance, oldschoolcool, philosophy, art, nosleep, writingprompts, creepy, \\\n",
    "               twoxchromosomes, fitness'\n",
    "popular_subs_2 = 'technology, wtf, bestof, adviceanimals, politics, atheism, interestingasfuck, europe, woahdude, \\\n",
    "                 blackpeopletwitter, oddlysatisfying, gonewild, leagueoflegends, pcmasterrace, reactiongifs, gameofthrones, \\\n",
    "                 wholesomememes, unexpected, overwatch, facepalm, trees, android, lifehacks, me_irl, relationships, games, \\\n",
    "                 nba, programming, tattoos, natureisfuckinglit, whatcouldgowrong, crappydesign, dankmemes, nsfw, cringepics, \\\n",
    "                 4chan, soccer, comics, sex, pokemon, malefashionadvice, nsfw_gif, starwars, frugal, historyporn, \\\n",
    "                 animalsbeingjerks, realgirls, travel, buildapc, outoftheloop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_subs_list = popular_subs.strip().replace(' ','').split(',')\n",
    "popular_subs_list_2 = popular_subs_2.strip().replace(' ','').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topofalltime_comments(sub, top_post_number):\n",
    "    \n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    post_ids = []\n",
    "    comments_list = []\n",
    "    \n",
    "    # getting post ids for all the top posts of a subreddit\n",
    "    for post in subreddit.top(limit = top_post_number):\n",
    "        post_ids.append(post.id)\n",
    "        \n",
    "    # appending top level comments of all top posts to a list\n",
    "    for i in post_ids:\n",
    "        counter = 0\n",
    "        for top_level_comment in reddit.submission(id=i).comments:\n",
    "#             time.sleep(1*random.random())\n",
    "            if counter < 50:\n",
    "                try:\n",
    "                    comments_list.append(top_level_comment.body)\n",
    "                    counter += 1\n",
    "                except:\n",
    "                    comments_list.append('null')\n",
    "                \n",
    "    return comments_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"But when I finally do, it'll be the years biggest shit ^^^^^so ^^^^^far\",\n",
       " \"I've been pissing out my ass all fuckin day.\",\n",
       " \"Bold of you to assume I'm going to take a shit this year.\",\n",
       " \"We have guests and I literally can't relax.\",\n",
       " \"I caught a diarrhea stomach bug and have been either in bed or on the toilet since about 7pm New Year's Eve. It's now 22 hours later and I'm still trying to let go of all of last year's shit. Someone send help.\",\n",
       " 'Quality shitpost ',\n",
       " \"It's all I have\",\n",
       " 'I’m letting go right now as I type. ',\n",
       " '\"Fuck you, I won\\'t poo when you tell me\"',\n",
       " 'Everyone now thinking about the last time they pooped. Nicely played. ',\n",
       " 'I do what I want, buddy.',\n",
       " 'i’m constipated \\n\\nEdit: Free Bobby Shmurda',\n",
       " 'Waiting for the Apple Store to open',\n",
       " 'Let’s see who can go the longest in 2019 without taking a shit\\n\\nEdit: This blew up..',\n",
       " 'My therapist says it’s because I have a hard time letting things go ',\n",
       " \"IT WON'T COME OUT. I need help :(\",\n",
       " 'I dont remember if I pooped',\n",
       " \"...huh.. i just realized i haven't pooped today.\\n\\nEdit: obligatory thanks for the gold n' sliver. And my most upvoted comment is about poopin'  #hashtag!!1! blessed\",\n",
       " 'I was actually sitting on the toilet at midnight.  New year, same shit.',\n",
       " 'I have trust issues ',\n",
       " 'I’ve poop twice since 2019 started, take that 2018! Your shit can’t hold me back any longer! ',\n",
       " 'I took Imodium for its labeled purpose yesterday evening and it is still working. ',\n",
       " 'I literally read this as I sat down for my first 2019 poop. ',\n",
       " 'Alcohol makes me constipated :P',\n",
       " \"I did an enema yesterday and didn't eat. 2019's first poop will be from food from 2019. Funny you should ask. \",\n",
       " 'cocaine',\n",
       " \"I don't poop at work\",\n",
       " \"Because, much like 2018, I don't give a crap in 2019.\",\n",
       " 'Woke up at 3am because I caught the stomach virus my daughter and my son had. \\n\\nI’ve lost like 4 pounds in the last 12 hours thanks to pooping and vomiting.\\n\\nWahoo, back under my pre-pregnancy weight. ',\n",
       " \"So I open up Reddit as I'm taking my 1st 2019 shit. Nice\",\n",
       " 'Ask this fucking annoying body of mine',\n",
       " \"I'm not ready to release the humongous 2018 crap out yet. Give us time\",\n",
       " \"It won't let go of ME!\",\n",
       " 'Then 2019 is official, right now, as I typed this...(grunt.. sigh) Happy New Year! ',\n",
       " 'Decided to go to the hospital for chest pains from2018. Hooked up to iv adavan and torrsomething. Maybe ill make it out of here alive or not. Im 40 bday in 5 days to 41. Weight 342. Non smoker no drugs or alchohol. Diet in 18 was fish chicken and veggies plus half gallon of water a day. No high b pressure. No diabietes no high ldl. I walked in to er.before coming in neededto buy groceries. Bought apples blueberries rasberrys plus 2 salmon steaks (for if i get home tonight).',\n",
       " 'Cause i only shit every 2-3 days',\n",
       " 'had a morning poop and an afternoon poop.  both very nice!',\n",
       " '10/10 quality shitpost',\n",
       " 'I pooped immediately after my 5 am shower. Real shitty way to start the new year.',\n",
       " 'Wow. By the time I was half way reading this headline I became disqualified from answering this question.\\n\\n\\n',\n",
       " 'The past haunts me no matter how hard i push.',\n",
       " 'Everybody remember to not pee for \"Don\\'t pee January\"',\n",
       " 'Day 27...I’m beginning to see the demons. ',\n",
       " 'Too much Turkey and stuffing and potatoes. My stomach is a brick.',\n",
       " 'I just unloaded ALL of last years shit. ',\n",
       " \"There's no TP.\",\n",
       " 'I had a Donner Kebab last night sunshine. No fucking way I could hold onto that for long. Launched this morning like a Chilean mudslide...',\n",
       " \"I'm busy and don't have time for this shit.\",\n",
       " \"Damn this post isn't even old enough to show the number for upvotes and it's on the front page with platinum\",\n",
       " 'I had a homemade edible last night, when it decides to break free my bathroom is going to smell like snoop dogg’s chill spot. ']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topofalltime_comments('askreddit', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_top_comments_all_subreddit(subredditlist):\n",
    "    for sub in subredditlist:\n",
    "        saving_dict = {}\n",
    "        saving_dict['subreddit'] = sub\n",
    "        saving_dict['comments'] = get_topofalltime_comments(sub, 20)\n",
    "        time.sleep(4+random.random())\n",
    "        \n",
    "        with open(str(sub)+'.json', 'w') as fp:\n",
    "            json.dump(saving_dict, fp)\n",
    "        print('Wow, we got the goods from ', sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow, we got the goods from  announcements\n",
      "Wow, we got the goods from  funny\n",
      "Wow, we got the goods from  askreddit\n",
      "Wow, we got the goods from  todayilearned\n",
      "Wow, we got the goods from  science\n",
      "Wow, we got the goods from  worldnews\n",
      "Wow, we got the goods from  pics\n",
      "Wow, we got the goods from  iama\n",
      "Wow, we got the goods from  gaming\n",
      "Wow, we got the goods from  videos\n",
      "Wow, we got the goods from  movies\n",
      "Wow, we got the goods from  aww\n",
      "Wow, we got the goods from  music\n",
      "Wow, we got the goods from  blog\n",
      "Wow, we got the goods from  gifs\n",
      "Wow, we got the goods from  news\n",
      "Wow, we got the goods from  explainlikeimfive\n",
      "Wow, we got the goods from  askscience\n",
      "Wow, we got the goods from  earthporn\n",
      "Wow, we got the goods from  books\n",
      "Wow, we got the goods from  television\n",
      "Wow, we got the goods from  mildlyinteresting\n",
      "Wow, we got the goods from  lifeprotips\n",
      "Wow, we got the goods from  showerthoughts\n",
      "Wow, we got the goods from  space\n",
      "Wow, we got the goods from  diy\n",
      "Wow, we got the goods from  jokes\n",
      "Wow, we got the goods from  gadgets\n",
      "Wow, we got the goods from  nottheonion\n",
      "Wow, we got the goods from  sports\n",
      "Wow, we got the goods from  tifu\n",
      "Wow, we got the goods from  food\n",
      "Wow, we got the goods from  photoshopbattles\n",
      "Wow, we got the goods from  documentaries\n",
      "Wow, we got the goods from  futureology\n",
      "Wow, we got the goods from  history\n",
      "Wow, we got the goods from  internetisbeautiful\n",
      "Wow, we got the goods from  dataisbeautiful\n",
      "Wow, we got the goods from  upliftingnews\n",
      "Wow, we got the goods from  listentothis\n",
      "Wow, we got the goods from  getmotivated\n",
      "Wow, we got the goods from  personalfinance\n",
      "Wow, we got the goods from  oldschoolcool\n",
      "Wow, we got the goods from  philosophy\n",
      "Wow, we got the goods from  art\n",
      "Wow, we got the goods from  nosleep\n",
      "Wow, we got the goods from  writingprompts\n",
      "Wow, we got the goods from  creepy\n",
      "Wow, we got the goods from  twoxchromosomes\n",
      "Wow, we got the goods from  fitness\n"
     ]
    }
   ],
   "source": [
    "save_top_comments_all_subreddit(popular_subs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow, we got the goods from  technology\n",
      "Wow, we got the goods from  wtf\n",
      "Wow, we got the goods from  bestof\n",
      "Wow, we got the goods from  adviceanimals\n",
      "Wow, we got the goods from  politics\n",
      "Wow, we got the goods from  atheism\n",
      "Wow, we got the goods from  interestingasfuck\n",
      "Wow, we got the goods from  europe\n",
      "Wow, we got the goods from  woahdude\n",
      "Wow, we got the goods from  blackpeopletwitter\n",
      "Wow, we got the goods from  oddlysatisfying\n",
      "Wow, we got the goods from  gonewild\n",
      "Wow, we got the goods from  leagueoflegends\n",
      "Wow, we got the goods from  pcmasterrace\n",
      "Wow, we got the goods from  reactiongifs\n",
      "Wow, we got the goods from  gameofthrones\n",
      "Wow, we got the goods from  wholesomememes\n",
      "Wow, we got the goods from  unexpected\n",
      "Wow, we got the goods from  overwatch\n",
      "Wow, we got the goods from  facepalm\n",
      "Wow, we got the goods from  trees\n",
      "Wow, we got the goods from  android\n",
      "Wow, we got the goods from  lifehacks\n",
      "Wow, we got the goods from  me_irl\n",
      "Wow, we got the goods from  relationships\n",
      "Wow, we got the goods from  games\n",
      "Wow, we got the goods from  nba\n",
      "Wow, we got the goods from  programming\n",
      "Wow, we got the goods from  tattoos\n",
      "Wow, we got the goods from  natureisfuckinglit\n",
      "Wow, we got the goods from  whatcouldgowrong\n",
      "Wow, we got the goods from  crappydesign\n",
      "Wow, we got the goods from  dankmemes\n",
      "Wow, we got the goods from  nsfw\n",
      "Wow, we got the goods from  cringepics\n",
      "Wow, we got the goods from  4chan\n",
      "Wow, we got the goods from  soccer\n",
      "Wow, we got the goods from  comics\n",
      "Wow, we got the goods from  sex\n",
      "Wow, we got the goods from  pokemon\n",
      "Wow, we got the goods from  malefashionadvice\n",
      "Wow, we got the goods from  nsfw_gif\n",
      "Wow, we got the goods from  starwars\n",
      "Wow, we got the goods from  frugal\n",
      "Wow, we got the goods from  historyporn\n",
      "Wow, we got the goods from  animalsbeingjerks\n",
      "Wow, we got the goods from  realgirls\n",
      "Wow, we got the goods from  travel\n",
      "Wow, we got the goods from  buildapc\n",
      "Wow, we got the goods from  outoftheloop\n"
     ]
    }
   ],
   "source": [
    "save_top_comments_all_subreddit(popular_subs_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function merges all the json files in my subreddit directory. Then finally it drops all the values in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this finds json files\n",
    "path_to_json = 'Top100/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "jsons_data = pd.DataFrame(columns=['subreddit', 'all_comments'])\n",
    "\n",
    "\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        \n",
    "    subreddit = json_text['subreddit']\n",
    "    comments = json_text['comments']\n",
    "    jsons_data.loc[index] = [subreddit, comments]\n",
    "\n",
    "df_top_100 = jsons_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>all_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>documentaries</td>\n",
       "      <td>[There's nothing hypothetical about what ISPs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fitness</td>\n",
       "      <td>[It feels like this was written specifically f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adviceanimals</td>\n",
       "      <td>[For fucks sake the previous post triggered AM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oddlysatisfying</td>\n",
       "      <td>[&gt; Hi I made this, my name is Lawrence Becker ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>music</td>\n",
       "      <td>[***Wanna do something about Net Neutrality??*...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subreddit                                       all_comments\n",
       "0    documentaries  [There's nothing hypothetical about what ISPs ...\n",
       "1          fitness  [It feels like this was written specifically f...\n",
       "2    adviceanimals  [For fucks sake the previous post triggered AM...\n",
       "3  oddlysatisfying  [> Hi I made this, my name is Lawrence Becker ...\n",
       "4            music  [***Wanna do something about Net Neutrality??*..."
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100 entries, 0 to 99\n",
      "Data columns (total 2 columns):\n",
      "subreddit       100 non-null object\n",
      "all_comments    100 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_top_100.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_100.to_json(r'subreddit_comments.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_data', 'wb') as to_write:\n",
    "    pickle.dump(df_top_100, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
